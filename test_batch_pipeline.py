"""
Batch Pipelineæµ‹è¯•è„šæœ¬

æµ‹è¯•batchå¤„ç†åŠŸèƒ½ï¼š
1. å•ä¸ªè¯·æ±‚å¤šturns
2. å¤šä¸ªè¯·æ±‚å¹¶å‘
3. æ˜¾å­˜è‡ªé€‚åº”batchå¤§å°
4. æ€§èƒ½å¯¹æ¯”ï¼ˆbatch vs sequentialï¼‰
"""
import os
import sys
import time
import logging
import argparse
from pathlib import Path

import torch
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from soulxpodcast.models.soulxpodcast import SoulXPodcast
from soulxpodcast.config import Config, SoulXPodcastLLMConfig, SamplingParams
from soulxpodcast.engine.batch_manager import TurnRequest
from soulxpodcast.models.soulxpodcast_batch import SoulXPodcastBatchPipeline

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_single_request_multi_turns(model_path: str):
    """
    æµ‹è¯•1: å•ä¸ªè¯·æ±‚å¤šä¸ªturns

    éªŒè¯batch pipelineèƒ½å¦æ­£ç¡®å¤„ç†ä¸€ä¸ªè¯·æ±‚çš„å¤šä¸ªturns
    """
    print("\n" + "=" * 80)
    print("æµ‹è¯•1: å•ä¸ªè¯·æ±‚å¤šä¸ªturns")
    print("=" * 80)

    # åŠ è½½é…ç½®
    hf_config = SoulXPodcastLLMConfig.from_json(f"{model_path}/soulxpodcast_config.json")
    config = Config(
        model=model_path,
        enforce_eager=True,
        llm_engine="vllm",
        hf_config=hf_config
    )

    # åŠ è½½æ¨¡å‹
    logger.info("Loading model...")
    model = SoulXPodcast(config)

    # åˆ›å»ºbatch pipeline
    batch_pipeline = SoulXPodcastBatchPipeline(
        model=model.llm,
        flow=model.flow,
        hift=model.hift,
        config=config,
        max_batch_size=16,
        enable_dynamic_batching=True,
    )

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    request_id = "test_req_001"
    num_turns = 5

    # åˆ›å»ºmock turn requests
    turn_requests = []
    for turn_id in range(num_turns):
        # Mock LLM input (ç®€åŒ–)
        llm_input = [1, 2, 3, 4, 5] * 10  # 50 tokens

        # Mock Flow inputs
        prompt_mel = torch.randn(1, 80, 100).cuda()
        prompt_mel_len = torch.tensor([100]).cuda()
        spk_emb = torch.randn(1, 192).cuda()

        turn_req = TurnRequest(
            request_id=request_id,
            turn_id=turn_id,
            total_turns=num_turns,
            llm_input=llm_input,
            prompt_mel=prompt_mel,
            prompt_mel_len=prompt_mel_len,
            spk_emb=spk_emb,
            spk_id=0,
            sampling_params=SamplingParams(
                temperature=0.6,
                top_p=0.9,
                top_k=100,
                max_tokens=100,
            )
        )
        turn_requests.append(turn_req)

    # æ·»åŠ è¯·æ±‚
    logger.info(f"Adding request with {num_turns} turns...")
    batch_pipeline.add_request(request_id, turn_requests)

    # å¤„ç†batch
    logger.info("Processing batch...")
    start_time = time.time()

    # æŒç»­å¤„ç†ç›´åˆ°å®Œæˆ
    while not batch_pipeline.request_manager.is_request_completed(request_id):
        processed = batch_pipeline.process_batch(timeout=0.5)
        if processed > 0:
            logger.info(f"Processed {processed} turns")
        time.sleep(0.1)

    total_time = time.time() - start_time

    # è·å–ç»“æœ
    results = batch_pipeline.get_request_results(request_id)

    # éªŒè¯ç»“æœ
    print(f"\nâœ“ æµ‹è¯•å®Œæˆ!")
    print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"  å¤„ç†turns: {len(results)}")
    print(f"  å¹³å‡è€—æ—¶: {total_time/num_turns:.2f}ç§’/turn")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = batch_pipeline.get_statistics()
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  Batchè°ƒåº¦å™¨:")
    print(f"    å½“å‰batchå¤§å°: {stats['scheduler']['current_batch_size']}")
    print(f"    æ€»batchæ•°: {stats['scheduler']['total_batches']}")
    print(f"    OOMæ¬¡æ•°: {stats['scheduler']['oom_count']}")
    print(f"  è¯·æ±‚ç®¡ç†å™¨:")
    print(f"    æ€»è¯·æ±‚æ•°: {stats['request_manager']['total_requests']}")
    print(f"    å·²å¤„ç†turns: {stats['request_manager']['total_turns_processed']}")


def test_multi_requests_concurrent(model_path: str, num_requests: int = 3):
    """
    æµ‹è¯•2: å¤šä¸ªè¯·æ±‚å¹¶å‘å¤„ç†

    éªŒè¯batch pipelineèƒ½å¦å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚
    """
    print("\n" + "=" * 80)
    print(f"æµ‹è¯•2: {num_requests}ä¸ªè¯·æ±‚å¹¶å‘å¤„ç†")
    print("=" * 80)

    # åŠ è½½é…ç½®
    hf_config = SoulXPodcastLLMConfig.from_json(f"{model_path}/soulxpodcast_config.json")
    config = Config(
        model=model_path,
        enforce_eager=True,
        llm_engine="vllm",
        hf_config=hf_config
    )

    # åŠ è½½æ¨¡å‹
    logger.info("Loading model...")
    model = SoulXPodcast(config)

    # åˆ›å»ºbatch pipeline
    batch_pipeline = SoulXPodcastBatchPipeline(
        model=model.llm,
        flow=model.flow,
        hift=model.hift,
        config=config,
        max_batch_size=32,
        enable_dynamic_batching=True,
    )

    # åˆ›å»ºå¤šä¸ªè¯·æ±‚
    all_request_ids = []
    for req_idx in range(num_requests):
        request_id = f"test_req_{req_idx:03d}"
        all_request_ids.append(request_id)

        num_turns = np.random.randint(3, 8)  # éšæœº3-7ä¸ªturns

        turn_requests = []
        for turn_id in range(num_turns):
            llm_input = [1, 2, 3, 4, 5] * 10

            prompt_mel = torch.randn(1, 80, 100).cuda()
            prompt_mel_len = torch.tensor([100]).cuda()
            spk_emb = torch.randn(1, 192).cuda()

            turn_req = TurnRequest(
                request_id=request_id,
                turn_id=turn_id,
                total_turns=num_turns,
                llm_input=llm_input,
                prompt_mel=prompt_mel,
                prompt_mel_len=prompt_mel_len,
                spk_emb=spk_emb,
                spk_id=req_idx % 2,  # äº¤æ›¿ä½¿ç”¨2ä¸ªè¯´è¯äºº
                sampling_params=SamplingParams(
                    temperature=0.6,
                    top_p=0.9,
                    top_k=100,
                    max_tokens=100,
                )
            )
            turn_requests.append(turn_req)

        batch_pipeline.add_request(request_id, turn_requests)
        logger.info(f"Added request {request_id} with {num_turns} turns")

    # å¤„ç†æ‰€æœ‰è¯·æ±‚
    logger.info("Processing all requests...")
    start_time = time.time()

    # æŒç»­å¤„ç†ç›´åˆ°æ‰€æœ‰è¯·æ±‚å®Œæˆ
    all_completed = False
    total_processed = 0

    while not all_completed:
        processed = batch_pipeline.process_batch(timeout=0.5)
        if processed > 0:
            total_processed += processed
            logger.info(f"Processed {processed} turns, total: {total_processed}")

        # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆ
        all_completed = all(
            batch_pipeline.request_manager.is_request_completed(req_id)
            for req_id in all_request_ids
        )

        time.sleep(0.1)

    total_time = time.time() - start_time

    # æ”¶é›†ç»“æœ
    all_results = {}
    for req_id in all_request_ids:
        results = batch_pipeline.get_request_results(req_id)
        all_results[req_id] = results

    # éªŒè¯ç»“æœ
    print(f"\nâœ“ æµ‹è¯•å®Œæˆ!")
    print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"  å¤„ç†è¯·æ±‚æ•°: {len(all_request_ids)}")
    print(f"  å¤„ç†turnsæ€»æ•°: {total_processed}")
    print(f"  å¹³å‡è€—æ—¶: {total_time/total_processed:.2f}ç§’/turn")

    # æ¯ä¸ªè¯·æ±‚çš„ç»“æœ
    print(f"\nğŸ“‹ å„è¯·æ±‚ç»“æœ:")
    for req_id, results in all_results.items():
        print(f"  {req_id}: {len(results)} turns completed")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = batch_pipeline.get_statistics()
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  Batchè°ƒåº¦å™¨:")
    print(f"    å½“å‰batchå¤§å°: {stats['scheduler']['current_batch_size']}")
    print(f"    æ€»batchæ•°: {stats['scheduler']['total_batches']}")
    print(f"  æ˜¾å­˜ä½¿ç”¨:")
    print(f"    å·²åˆ†é…: {stats['scheduler']['allocated']:.2f} GB")
    print(f"    æ€»æ˜¾å­˜: {stats['scheduler']['total']:.2f} GB")


def test_performance_comparison(model_path: str):
    """
    æµ‹è¯•3: æ€§èƒ½å¯¹æ¯” (Batch vs Sequential)

    å¯¹æ¯”batchå¤„ç†å’Œé¡ºåºå¤„ç†çš„æ€§èƒ½å·®å¼‚
    """
    print("\n" + "=" * 80)
    print("æµ‹è¯•3: æ€§èƒ½å¯¹æ¯” (Batch vs Sequential)")
    print("=" * 80)

    print("âš ï¸  æ­¤æµ‹è¯•éœ€è¦å®é™…æ¨¡å‹ï¼Œå½“å‰ä¸ºå ä½å®ç°")
    print("   åœ¨çœŸå®ç¯å¢ƒä¸­è¿è¡Œæ—¶ä¼šå¯¹æ¯”ï¼š")
    print("   - Sequential: é€ä¸ªå¤„ç†turns")
    print("   - Batch: æ‰¹é‡å¤„ç†turns")
    print("   é¢„æœŸåŠ é€Ÿæ¯”: 2-5x (å–å†³äºbatchå¤§å°)")


def main():
    parser = argparse.ArgumentParser(description="Batch Pipelineæµ‹è¯•")
    parser.add_argument(
        "--model-path",
        type=str,
        default="/path/to/model",
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--test",
        type=str,
        choices=["single", "multi", "performance", "all"],
        default="single",
        help="æµ‹è¯•ç±»å‹"
    )
    parser.add_argument(
        "--num-requests",
        type=int,
        default=3,
        help="å¹¶å‘è¯·æ±‚æ•°ï¼ˆmultiæµ‹è¯•ç”¨ï¼‰"
    )

    args = parser.parse_args()

    print("=" * 80)
    print("SoulXPodcast Batch Pipeline æµ‹è¯•")
    print("=" * 80)
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æµ‹è¯•ç±»å‹: {args.test}")

    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        print(f"\nâœ“ CUDAå¯ç”¨")
        print(f"  è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        print(f"\nâš ï¸  CUDAä¸å¯ç”¨ï¼Œæµ‹è¯•å¯èƒ½å¤±è´¥")

    # è¿è¡Œæµ‹è¯•
    try:
        if args.test in ["single", "all"]:
            test_single_request_multi_turns(args.model_path)

        if args.test in ["multi", "all"]:
            test_multi_requests_concurrent(args.model_path, args.num_requests)

        if args.test in ["performance", "all"]:
            test_performance_comparison(args.model_path)

        print("\n" + "=" * 80)
        print("âœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("=" * 80)

    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        print(f"\nâœ— æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
